ç©ºæ°” æ°§æ°”å±äºç»„åˆä¹Ÿå¯ä»¥æ˜¯ç»§æ‰¿å…³ç³»

èšåˆ 1300
å…³è” 1000
ç»„åˆ 1000
ä¾èµ– 600
ç»§æ‰¿ 2200

æ£€ç´¢3class_dependcyå­ç›®å½•ä¸‹çš„æ‰€æœ‰data.txt,ä»–ä»¬æ˜¯å…·æœ‰ä¾èµ–å…³ç³»çš„ä¸‰å…ƒç»„ï¼Œæ¯”å¦‚ä¸‰å…ƒç»„a b cï¼Œç¬¦åˆa ä¾èµ–b bä¾èµ–c

æ•°æ®é‡Œæœ‰å¤ªå¤šé‡å¤äº†ï¼Œæ¯”å¦‚ä¸‰å…ƒç»„a b cé‡Œå¤ªå¤š(b,c)æ˜¯é‡å¤çš„äº†ï¼Œæˆ‘æœ€å¤šè¦æ±‚é‡å¤bï¼Œcåªèƒ½å‡ºç°ä¸‰æ¬¡ï¼Œæˆ‘è¦æ±‚ä½ å°†æ•°é‡è¶…è¿‡3ä¸ªçš„é‡å¤çš„ä¸‰å…ƒç»„æŒ‰ç…§æ˜¯å¦æœ€ç¬¦åˆä¾èµ–å…³ç³»ä»¥åŠæ˜¯å¦æ˜¯çœŸå®å­˜åœ¨çš„äº‹ç‰©ï¼Œä¸èƒ½èƒ¡ç¼–ä¹±é€ æˆ–è€…ä½¿ç”¨æ‹¼æ¥è¯è¯­è¿›è¡Œæ’åºï¼Œåªä¿ç•™æœ€ç¬¦åˆçš„ä¸‰ä¸ªã€‚

æ­¤å¤–ï¼Œç°åœ¨æˆ‘è¿˜è¦æ±‚ä½ å°†æˆ‘çš„è¯¥ç›®å½•ä¸‹çš„æ•°æ®é‡å¡«è¡¥è‡³1000ä¸ªï¼Œå¡«è¡¥è‡³åˆé€‚çš„ç›®å½•data.txtä¸­ï¼Œå¦‚æœæ²¡æœ‰åˆé€‚çš„ç›®å½•ï¼Œåˆ™åˆ›å»ºï¼Œè¯·ä½ æŒ‰ç…§è¦æ±‚ç”Ÿæˆä¸‰å…ƒç»„çš„ä¾èµ–å…³ç³»a b cï¼Œæœ‰ä»¥ä¸‹è¦æ±‚ï¼š 
1. ä¸èƒ½ä½¿ç”¨æ ¹æœ¬ä¸å­˜åœ¨çš„æ‹¼æ¥è¯è¯­ï¼Œè‹¥è¦ä½¿ç”¨ï¼Œå¿…é¡»ç¡®ä¿å®ƒä»¬æ˜¯çœŸå®å­˜åœ¨çš„äº‹ç‰©
2. ç¡®ä¿å®ƒä»¬å®Œç¾ç¬¦åˆaä¾èµ–b bä¾èµ–c, å¿…é¡»æ»¡è¶³é€»è¾‘æ­£ç¡®æ€§ï¼ˆå¸¸è¯†è¯­ä¹‰ï¼‰
3. å¢åŠ çš„ä¸‰å…ƒç»„a b cé‡Œï¼Œ(b,c)æœ€å¤šåªèƒ½å‡ºç°ä¸‰æ¬¡ï¼Œæ„æ€æ˜¯ä½ ä¸èƒ½åªæ›¿æ¢a ç„¶åb cä¸€ç›´ä¸å˜

-------

æ£€ç´¢æ‰€æœ‰3classå¼€å¤´ç›®å½•ä¸‹çš„æ‰€æœ‰data.txt,ä»–ä»¬æ˜¯å…·æœ‰å„ç§å…³ç³»çš„ä¸‰å…ƒç»„ï¼Œè¯¥æ–‡ä»¶å¤¹çš„æ‰€æœ‰å…³ç³»ç±»å‹å¿…é¡»æ»¡è¶³æ–‡ä»¶å¤¹ä¸­çš„åå­—ï¼Œæ¯”å¦‚3class_inheritanceä¸­çš„æ‰€æœ‰data.txté‡Œçš„æ‰€æœ‰ä¸‰å…ƒç»„å¿…é¡»æ»¡è¶³ç»§æ‰¿å…³ç³»æ¯”å¦‚aç»§æ‰¿b bç»§æ‰¿ccï¼Œ åˆæ¯”å¦‚3class_Dependencyä¸­çš„ä¸‰å…ƒç»„a b cï¼Œå¿…é¡»è¦ç¬¦åˆa ä¾èµ–b bä¾èµ–c

å¦‚æœä¸æ˜¯å®Œå…¨ç¬¦åˆè¦æ±‚æˆ–è€…å‹‰å¼ºç¬¦åˆè¦æ±‚éƒ½è¦ä½œä¸ºè¢«è¿‡æ»¤çš„å¯¹è±¡ï¼Œæœ€åè¿”å›è¿‡æ»¤åçš„ç»“æœä»¥åŠå“ªäº›ä¸‰å…ƒç»„è¦è¢«è¿‡æ»¤ã€‚æœ‰ä»¥ä¸‹è¦æ±‚ï¼š 
1. ä¸èƒ½ä½¿ç”¨æ ¹æœ¬ä¸å­˜åœ¨çš„æ‹¼æ¥è¯è¯­ï¼Œè‹¥è¦ä½¿ç”¨ï¼Œå¿…é¡»ç¡®ä¿å®ƒä»¬æ˜¯çœŸå®å­˜åœ¨çš„äº‹ç‰©ï¼Œè™šæ„çš„ç”µå½±å’Œå…¶ä¸­çš„åå­—ã€éŸ³ä¹ç­‰éƒ½å¯ä»¥ä½¿ç”¨ï¼Œä½†ä¸èƒ½ä½¿ç”¨æ‹¼æ¥è¯è¯­æ¯”å¦‚â€œç”µå½±ä¹¦ç±â€æˆ–è€…â€œåŠ¨ç‰©æ¤ç‰©â€ï¼Œå¿…é¡»æ˜¯å…·ä½“çš„ç”µå½±çš„åå­—
2. ç¡®ä¿å®ƒä»¬å®Œç¾ç¬¦åˆa->b b->c, å¿…é¡»æ»¡è¶³é€»è¾‘æ­£ç¡®æ€§ï¼ˆå¸¸è¯†è¯­ä¹‰ï¼‰ï¼Œå…·ä½“æ–¹å‘å‚è€ƒdata.txté‡Œå·²ç»å­˜åœ¨çš„ä¸‰å…ƒç»„
3. å¢åŠ çš„ä¸‰å…ƒç»„a b cé‡Œï¼Œ(b,c)æœ€å¤šåªèƒ½å‡ºç°ä¸‰æ¬¡ï¼Œæ„æ€æ˜¯ä½ ä¸èƒ½åªæ›¿æ¢a ç„¶åb cä¸€ç›´ä¸å˜
4. å¦‚æœä½ ä¸ç¡®å®šæŸä¸ªä¸‰å…ƒç»„æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œæˆ–è€…æ˜¯å¦çœŸå®å­˜åœ¨çš„äº‹ç‰©ï¼Œè¯·ä¸è¦åˆ é™¤å®ƒï¼Œä¿æŒè°¨æ…æ€åº¦ã€‚

https://drive.google.com/drive/folders/1rVVjDFjZaZYyeeaKFIb1T8HdJn8f52ci?dmr=1&ec=wgc-drive-%5Bmodule%5D-goto
# required:
#   --model_name
#   --dataset
# optional; experiment with these:
#   --learning_rate, --max_seq_length, --per_device_train_batch_size, --gradient_accumulation_steps, --max_steps
# to save the model at the end of training:
#   --save_model

torchrun --nproc_per_node=2 unsloth-cli.py \
  --model_name=Qwen/Qwen3-8B \
  --dataset=yahma/alpaca-cleaned \
  --learning_rate=2e-5 \
  --max_seq_length=2048 \
  --per_device_train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_steps=1000 \
  --save_model

  $ python unsloth-cli.py --help
usage: unsloth-cli.py [-h] [--model_name MODEL_NAME] [--max_seq_length MAX_SEQ_LENGTH] [--dtype DTYPE]
                      [--load_in_4bit] [--dataset DATASET] [--r R] [--lora_alpha LORA_ALPHA]
                      [--lora_dropout LORA_DROPOUT] [--bias BIAS]
                      [--use_gradient_checkpointing USE_GRADIENT_CHECKPOINTING]
â€¦

ğŸ¦¥ Fine-tune your llm faster using unsloth!

options:
  -h, --help            show this help message and exit

ğŸ¤– Model Options:
  --model_name MODEL_NAME
                        Model name to load
  --max_seq_length MAX_SEQ_LENGTH
                        Maximum sequence length, default is 2048. We auto support RoPE Scaling
                        internally!
â€¦

ğŸ§  LoRA Options:
  These options are used to configure the LoRA model.

  --r R                 Rank for Lora model, default is 16. (common values: 8, 16, 32, 64, 128)
  --lora_alpha LORA_ALPHA
                        LoRA alpha parameter, default is 16. (common values: 8, 16, 32, 64, 128)
â€¦

ğŸ“ Training Options:
  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE
                        Batch size per device during training, default is 2.
  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE
                        Batch size per device during evaluation, default is 4.
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of gradient accumulation steps, default is 4.
â€¦